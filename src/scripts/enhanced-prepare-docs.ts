import { getChunkedDocsFromPDF, getChunkedDocsIncrementally } from "@/lib/pdf-loader";
import { processMalayalamDocuments } from "@/lib/malayalam-pinecone-processor";
import { env } from "@/lib/env";

/**
 * Enhanced document preparation script using Docling-inspired chunking and OpenAI embeddings
 * This script mimics the Python LanceDB approach but uses Pinecone as the vector store
 */

const mode = process.argv[2] || 'incremental';

(async () => {
    try {
        console.log("ğŸš€ Starting Enhanced Document Preparation with Docling-inspired Chunking");
        console.log("=".repeat(70));

        console.log("âœ… Malayalam processor will handle chunking and storage");

        if (mode === 'full' || mode === 'legacy') {
            console.log("\nğŸ”„ Running in FULL REBUILD mode with Enhanced Processing...");
            console.log("âš ï¸  This will process ALL documents with Docling-inspired chunking");

            console.log("\nğŸ“„ Loading and chunking documents with hybrid chunker...");
            const docs = await getChunkedDocsFromPDF();
            console.log(`ğŸ“Š Loaded ${docs.length} markdown documents`);

            // Enhanced document analysis
            console.log("\nğŸ” Analyzing document composition...");
            const documentTypes = new Map<string, number>();
            const chunkTypes = new Map<string, number>();
            let totalTokens = 0;

            docs.forEach(doc => {
                const fileType = doc.metadata.fileType || 'unknown';
                const chunkType = doc.metadata.chunkType || 'text';

                documentTypes.set(fileType, (documentTypes.get(fileType) || 0) + 1);
                chunkTypes.set(chunkType, (chunkTypes.get(chunkType) || 0) + 1);

                if (doc.metadata.tokenCount) {
                    totalTokens += doc.metadata.tokenCount as number;
                }
            });

            console.log("\nğŸ“ˆ Document Analysis Results:");
            console.log(`ğŸ“‹ File Types: ${Array.from(documentTypes.entries()).map(([type, count]) => `${type}: ${count}`).join(', ')}`);
            console.log(`ğŸ§© Chunk Types: ${Array.from(chunkTypes.entries()).map(([type, count]) => `${type}: ${count}`).join(', ')}`);
            console.log(`ğŸ”¤ Total Tokens: ${totalTokens.toLocaleString()}`);

            console.log("\nğŸš€ Processing and storing documents via Malayalam processor...");
            const result = await processMalayalamDocuments(
                docs.map(d => ({ content: d.pageContent, filename: d.metadata.source || 'unknown.md', source: d.metadata.source || 'unknown' })),
                { namespace: env.PINECONE_NAMESPACE || 'malayalam-docs', enforceLanguage: false }
            );
            console.log("âœ… Stored:", result);

            console.log("\nâœ… Enhanced document preparation completed successfully!");

        } else {
            console.log("\nğŸ”„ Running in INCREMENTAL mode with Enhanced Processing...");
            console.log("âœ… Only new or changed documents will be processed with enhanced chunking");

            const result = await getChunkedDocsIncrementally({});

            if (result.documents.length > 0) {
                console.log(`\nğŸ”„ Processing ${result.documents.length} new/updated chunks with enhanced embedding...`);
                const store = await processMalayalamDocuments(
                    result.documents.map(d => ({ content: d.pageContent, filename: d.metadata.source || 'unknown.md', source: d.metadata.source || 'unknown' })),
                    { namespace: env.PINECONE_NAMESPACE || 'malayalam-docs', enforceLanguage: false }
                );

                console.log('\nğŸ“Š ENHANCED INCREMENTAL UPDATE SUMMARY:');
                console.log('='.repeat(60));
                console.log(`ğŸ“ˆ Total Documents: ${result.updateResult.totalDocuments}`);
                console.log(`â• New Documents: ${result.updateResult.newDocuments}`);
                console.log(`ğŸ”„ Updated Documents: ${result.updateResult.updatedDocuments}`);
                console.log(`â­ï¸  Skipped Documents: ${result.updateResult.skippedDocuments}`);
                console.log(`ğŸ“¦ Processed Chunks: ${result.updateResult.processedChunks}`);
                console.log(`â±ï¸  Processing Time: ${(result.updateResult.processingTime / 1000).toFixed(2)}s`);
                console.log('='.repeat(60));
            } else {
                console.log("âœ¨ No new or updated documents found - all data is up to date!");
            }
        }

        console.log("\nğŸ‰ Enhanced Processing completed successfully!");

        // Show enhanced usage tips
        console.log("\nğŸ’¡ Enhanced Features Added:");
        console.log("âœ¨ Docling-inspired hierarchical chunking");
        console.log("ğŸ”¤ Accurate token counting with tiktoken");
        console.log("ğŸ§  Smart peer chunk merging");
        console.log("ğŸ“Š Enhanced metadata with semantic tags");
        console.log("ğŸ¯ Quality scoring for chunks");
        console.log("ğŸ·ï¸  Dynamic namespacing by content type");

        console.log("\nğŸ”§ Usage Commands:");
        console.log("- npm run enhanced:prepare - Incremental updates (recommended)");
        console.log("- npm run enhanced:prepare full - Complete rebuild");

    } catch (error) {
        console.error("âŒ Enhanced document preparation failed:", error);

        // Enhanced error reporting
        if (error instanceof Error) {
            console.error("Error details:", error.message);
            if (error.stack) {
                console.error("Stack trace:", error.stack);
            }
        }

        process.exit(1);
    }
})();
